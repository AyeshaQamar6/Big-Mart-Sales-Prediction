# -*- coding: utf-8 -*-
"""Big-Mart Sales Data Prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1skHLwlwwmX5Q1Nx8TtrKak72SXfKxdAh
"""

from google.colab import drive
drive.mount('/content/drive')

"""'''BigMart.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1skHLwlwwmX5Q1Nx8TtrKak72SXfKxdAh#scrollTo=kiaVf8HNa1D8
'''
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn import metrics

from xgboost import XGBRegressor

# Loading Data

data=pd.read_csv('/content/drive/MyDrive/BigData Sales.csv')
data.head()

data.shape

data.info()

# Rows and columns
print(f'Training Dataset (row, col): {data.shape}')

"""Item_Weight and Outlet_Size are short of null values"""
data.isnull().sum()

#filling Item_weight Null values 

data['Item_Weight'].mean()
data['Item_Weight'].fillna(data['Item_Weight'].mean(), inplace=True)
data.isnull().sum()

#filling Outlet_Size

mode_OutletSize = data.pivot_table(values='Outlet_Size', columns='Outlet_Type', aggfunc=(lambda x: x.mode()[0]))
print(mode_OutletSize)
missing_val= data['Outlet_Size'].isnull()
print(missing_val)
data.loc[missing_val,'Outlet_Size']= data.loc[missing_val, 'Outlet_Type'].apply(lambda x:mode_OutletSize[x])
data.isnull().sum()

#Analysis

data.describe()

plt.figure(figsize=(6,6))
sns.displot(data['Item_Weight'])
plt.show();

plt.figure(figsize=(6,6))
sns.displot(data['Item_Visibility'])
plt.show();

plt.figure(figsize=(6,6))
sns.displot(data['Item_MRP'])
plt.show();

plt.figure(figsize=(6,6))
sns.displot(data['Item_Outlet_Sales'])
plt.show();

plt.figure(figsize=(6,6))
sns.countplot(x='Outlet_Establishment_Year', data= data)
plt.show();

plt.figure(figsize=(6,6))
sns.countplot(x='Item_Fat_Content', data= data)
plt.show();

plt.figure(figsize=(25,10))
sns.countplot(x='Item_Type', data= data)
plt.show();

plt.figure(figsize=(10,6))
sns.countplot(x='Outlet_Type', data= data)
plt.show();

plt.figure(figsize=(10,6))
sns.countplot(x='Outlet_Size', data= data)
plt.show();

# Building the correlation matrix
sns.heatmap(data.corr())

#Mart.Outlet_Size.dtype

data.info()

# Checking missing Values
print('Missing Values by Count: \n\n',
      data.isnull().sum().sort_values(ascending=False),'\n\nMissing Values by %:\n\n',
      data.isnull().sum().sort_values(ascending=False)/data.shape[0] * 100)

#Preprocessing

data['Item_Fat_Content'].value_counts()
data.replace({'Item_Fat_Content':{'low fat':'Low Fat','LF':'Low Fat', 'reg':'Regular'}}, inplace=True)
data['Item_Fat_Content'].value_counts()
encoder = LabelEncoder()

data['Item_Identifier'] = encoder.fit_transform(data['Item_Identifier'])
data['Item_Fat_Content'] = encoder.fit_transform(data['Item_Fat_Content'])
data['Item_Type'] = encoder.fit_transform(data['Item_Type'])
data['Outlet_Identifier'] = encoder.fit_transform(data['Outlet_Identifier'])
data['Outlet_Size'] = encoder.fit_transform(data['Outlet_Size'])
data['Outlet_Location_Type'] = encoder.fit_transform(data['Outlet_Location_Type'])
data['Outlet_Type'] = encoder.fit_transform(data['Outlet_Type'])
data

Mart = data.dropna()

Mart.info(verbose=True, show_counts=True)

#spliting

X= data.drop(columns='Item_Outlet_Sales', axis=1)
Y= data['Item_Outlet_Sales']
print(X)
print(Y)
X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2, random_state=2)

#Model Training

Model=XGBRegressor(tree_method='auto', objective='reg:squarederror')
Model.fit(X_train,Y_train)
training_prediction=Model.predict(X_train)
r2_train_score=metrics.r2_score(Y_train,training_prediction)
print(r2_train_score)
test_prediction=Model.predict(X_test)
r2_test_score=metrics.r2_score(Y_test,test_prediction)
print('Test Score:', r2_test_score)

